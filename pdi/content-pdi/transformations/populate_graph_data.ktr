<?xml version="1.0" encoding="UTF-8"?>
<transformation>
  <info>
    <name>Populate Platform Graph Data</name>
    <description />
    <extended_description />
    <trans_version />
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>/</directory>
    <parameters>
      <parameter>
        <name>CHAIN_ID</name>
        <default_value />
        <description />
      </parameter>
      <parameter>
        <name>ENV_SUFFIX</name>
        <default_value />
        <description />
      </parameter>
      <parameter>
        <name>SUB_ENV</name>
        <default_value />
        <description />
      </parameter>
    </parameters>
    <log>
      <trans-log-table>
        <connection />
        <schema />
        <table>${KETTLE_TRANS_LOG_TABLE}_${CHAIN_ID}</table>
        <size_limit_lines />
        <interval />
        <timeout_days>${TRANSFORMATION_LOGS_RECORD_TIMEOUT}</timeout_days>
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STATUS</id>
          <enabled>Y</enabled>
          <name>STATUS</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
          <subject />
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
          <subject />
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
          <subject />
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
          <subject />
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
          <subject />
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>STARTDATE</id>
          <enabled>Y</enabled>
          <name>STARTDATE</name>
        </field>
        <field>
          <id>ENDDATE</id>
          <enabled>Y</enabled>
          <name>ENDDATE</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>DEPDATE</id>
          <enabled>Y</enabled>
          <name>DEPDATE</name>
        </field>
        <field>
          <id>REPLAYDATE</id>
          <enabled>Y</enabled>
          <name>REPLAYDATE</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>Y</enabled>
          <name>LOG_FIELD</name>
        </field>
        <field>
          <id>EXECUTING_SERVER</id>
          <enabled>N</enabled>
          <name>EXECUTING_SERVER</name>
        </field>
        <field>
          <id>EXECUTING_USER</id>
          <enabled>N</enabled>
          <name>EXECUTING_USER</name>
        </field>
        <field>
          <id>CLIENT</id>
          <enabled>N</enabled>
          <name>CLIENT</name>
        </field>
      </trans-log-table>
      <perf-log-table>
        <connection />
        <schema />
        <table />
        <interval />
        <timeout_days />
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>SEQ_NR</id>
          <enabled>Y</enabled>
          <name>SEQ_NR</name>
        </field>
        <field>
          <id>LOGDATE</id>
          <enabled>Y</enabled>
          <name>LOGDATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>INPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>INPUT_BUFFER_ROWS</name>
        </field>
        <field>
          <id>OUTPUT_BUFFER_ROWS</id>
          <enabled>Y</enabled>
          <name>OUTPUT_BUFFER_ROWS</name>
        </field>
      </perf-log-table>
      <channel-log-table>
        <connection />
        <schema />
        <table />
        <timeout_days />
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>LOGGING_OBJECT_TYPE</id>
          <enabled>Y</enabled>
          <name>LOGGING_OBJECT_TYPE</name>
        </field>
        <field>
          <id>OBJECT_NAME</id>
          <enabled>Y</enabled>
          <name>OBJECT_NAME</name>
        </field>
        <field>
          <id>OBJECT_COPY</id>
          <enabled>Y</enabled>
          <name>OBJECT_COPY</name>
        </field>
        <field>
          <id>REPOSITORY_DIRECTORY</id>
          <enabled>Y</enabled>
          <name>REPOSITORY_DIRECTORY</name>
        </field>
        <field>
          <id>FILENAME</id>
          <enabled>Y</enabled>
          <name>FILENAME</name>
        </field>
        <field>
          <id>OBJECT_ID</id>
          <enabled>Y</enabled>
          <name>OBJECT_ID</name>
        </field>
        <field>
          <id>OBJECT_REVISION</id>
          <enabled>Y</enabled>
          <name>OBJECT_REVISION</name>
        </field>
        <field>
          <id>PARENT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>PARENT_CHANNEL_ID</name>
        </field>
        <field>
          <id>ROOT_CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>ROOT_CHANNEL_ID</name>
        </field>
      </channel-log-table>
      <step-log-table>
        <connection />
        <schema />
        <table />
        <timeout_days />
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>TRANSNAME</id>
          <enabled>Y</enabled>
          <name>TRANSNAME</name>
        </field>
        <field>
          <id>STEPNAME</id>
          <enabled>Y</enabled>
          <name>STEPNAME</name>
        </field>
        <field>
          <id>STEP_COPY</id>
          <enabled>Y</enabled>
          <name>STEP_COPY</name>
        </field>
        <field>
          <id>LINES_READ</id>
          <enabled>Y</enabled>
          <name>LINES_READ</name>
        </field>
        <field>
          <id>LINES_WRITTEN</id>
          <enabled>Y</enabled>
          <name>LINES_WRITTEN</name>
        </field>
        <field>
          <id>LINES_UPDATED</id>
          <enabled>Y</enabled>
          <name>LINES_UPDATED</name>
        </field>
        <field>
          <id>LINES_INPUT</id>
          <enabled>Y</enabled>
          <name>LINES_INPUT</name>
        </field>
        <field>
          <id>LINES_OUTPUT</id>
          <enabled>Y</enabled>
          <name>LINES_OUTPUT</name>
        </field>
        <field>
          <id>LINES_REJECTED</id>
          <enabled>Y</enabled>
          <name>LINES_REJECTED</name>
        </field>
        <field>
          <id>ERRORS</id>
          <enabled>Y</enabled>
          <name>ERRORS</name>
        </field>
        <field>
          <id>LOG_FIELD</id>
          <enabled>N</enabled>
          <name>LOG_FIELD</name>
        </field>
      </step-log-table>
      <metrics-log-table>
        <connection />
        <schema />
        <table />
        <timeout_days />
        <field>
          <id>ID_BATCH</id>
          <enabled>Y</enabled>
          <name>ID_BATCH</name>
        </field>
        <field>
          <id>CHANNEL_ID</id>
          <enabled>Y</enabled>
          <name>CHANNEL_ID</name>
        </field>
        <field>
          <id>LOG_DATE</id>
          <enabled>Y</enabled>
          <name>LOG_DATE</name>
        </field>
        <field>
          <id>METRICS_DATE</id>
          <enabled>Y</enabled>
          <name>METRICS_DATE</name>
        </field>
        <field>
          <id>METRICS_CODE</id>
          <enabled>Y</enabled>
          <name>METRICS_CODE</name>
        </field>
        <field>
          <id>METRICS_DESCRIPTION</id>
          <enabled>Y</enabled>
          <name>METRICS_DESCRIPTION</name>
        </field>
        <field>
          <id>METRICS_SUBJECT</id>
          <enabled>Y</enabled>
          <name>METRICS_SUBJECT</name>
        </field>
        <field>
          <id>METRICS_TYPE</id>
          <enabled>Y</enabled>
          <name>METRICS_TYPE</name>
        </field>
        <field>
          <id>METRICS_VALUE</id>
          <enabled>Y</enabled>
          <name>METRICS_VALUE</name>
        </field>
      </metrics-log-table>
    </log>
    <maxdate>
      <connection />
      <table />
      <field />
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>Y</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file />
    <capture_step_performance>N</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
    <created_user>-</created_user>
    <created_date>2019/03/28 15:59:20.653</created_date>
    <modified_user>-</modified_user>
    <modified_date>2019/03/28 15:59:20.653</modified_date>
    <key_for_session_key>H4sIAAAAAAAAAAMAAAAAAAAAAAA=</key_for_session_key>
    <is_key_private>N</is_key_private>
  </info>
  <notepads>
  </notepads>
  <connection>
    <name>analytics_mysql_connection</name>
    <server />
    <type>MYSQL</type>
    <access>JNDI</access>
    <database>analyticsMysqlConnection_${SUB_ENV}${ENV_SUFFIX}</database>
    <port>1521</port>
    <username />
    <password>Encrypted </password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>INITIAL_POOL_SIZE</code>
        <attribute>10</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>MAXIMUM_POOL_SIZE</code>
        <attribute>20</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>1521</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>STREAM_RESULTS</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>Y</attribute>
      </attribute>
    </attributes>
  </connection>
  <connection>
    <name>analytics_redshift_connection</name>
    <server />
    <type>REDSHIFT</type>
    <access>JNDI</access>
    <database>analyticsRedshiftConnection${ENV_SUFFIX}</database>
    <port>1521</port>
    <username />
    <password>Encrypted </password>
    <servername />
    <data_tablespace />
    <index_tablespace />
    <attributes>
      <attribute>
        <code>EXTRA_OPTION_REDSHIFT.tcpKeepAlive</code>
        <attribute>true</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_LOWERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>FORCE_IDENTIFIERS_TO_UPPERCASE</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>INITIAL_POOL_SIZE</code>
        <attribute>5</attribute>
      </attribute>
      <attribute>
        <code>IS_CLUSTERED</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>MAXIMUM_POOL_SIZE</code>
        <attribute>10</attribute>
      </attribute>
      <attribute>
        <code>PORT_NUMBER</code>
        <attribute>1521</attribute>
      </attribute>
      <attribute>
        <code>PRESERVE_RESERVED_WORD_CASE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>QUOTE_ALL_FIELDS</code>
        <attribute>N</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_BOOLEAN_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>SUPPORTS_TIMESTAMP_DATA_TYPE</code>
        <attribute>Y</attribute>
      </attribute>
      <attribute>
        <code>USE_POOLING</code>
        <attribute>Y</attribute>
      </attribute>
    </attributes>
  </connection>
  <order>
    <hop>
      <from>Get Timestamp</from>
      <to>insert_temp_graph_data_meta_type</to>
      <enabled>Y</enabled>
    </hop>
    <hop>
      <from>Get Timestamp</from>
      <to>insert_temp_graph_data_meta_name</to>
      <enabled>Y</enabled>
    </hop>
  </order>
  <step>
    <name>Get Timestamp</name>
    <type>TableInput</type>
    <description />
    <distribute>N</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>analytics_redshift_connection</connection>
    <sql>select  rounded_current_date_timestamp, rounded_current_time_timestamp from (
SELECT (3600 *((MOD(ct,86400) / 3600)::INTEGER)) AS rounded_current_time_timestamp,
       (ct-MOD(ct,86400)::INTEGER) AS rounded_current_date_timestamp
FROM (SELECT EXTRACT(epoch FROM GETDATE ()) AS ct));</sql>
    <limit>0</limit>
    <lookup />
    <execute_each_row>N</execute_each_row>
    <variables_active>Y</variables_active>
    <lazy_conversion_active>N</lazy_conversion_active>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>160</xloc>
      <yloc>112</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>insert_temp_graph_data_meta_name</name>
    <type>ExecSQL</type>
    <description />
    <distribute>N</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>analytics_mysql_connection</connection>
    <execute_each_row>Y</execute_each_row>
    <single_statement>N</single_statement>
    <replace_variables>Y</replace_variables>
    <quoteString>N</quoteString>
    <sql>
DELETE from transaction_by_name_graph where chain_id=${CHAIN_ID};

insert into transaction_by_name_graph(chain_id, token_id, `timestamp`, meta_name, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	(dd.timestamp + dt.timestamp) as final_timestamp,
	dmn.meta_name as meta_name,
	'day' as graph_duration_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_times dt ON ttf.time_sk = dt.time_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_names_${CHAIN_ID} as dmn on (dmn.meta_name_sk = ttf.meta_name_sk and ttf.meta_name_sk >0)
WHERE (dd.timestamp = ?)
OR    (dd.timestamp = (? - 86400) AND dt.timestamp >= (? + 3600))
and status = 'success'
group by 1,2,3,4,5
);



insert into transaction_by_name_graph(chain_id, token_id, `timestamp`, meta_name, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	dd.timestamp as final_timestamp,
	dmn.meta_name as meta_name,
	'week' as graph_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_202 AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
    left join dim_meta_names_${CHAIN_ID} as dmn on (dmn.meta_name_sk = ttf.meta_name_sk and ttf.meta_name_sk >0)	
WHERE (dd.timestamp > (? - (86400*7)))
and status = 'success'
group by 1,2,3,4,5
);


insert into transaction_by_name_graph(chain_id, token_id, `timestamp`, meta_name, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	dd.timestamp as final_timestamp,
	dmn.meta_name as meta_name,
	'month' as graph_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_names_${CHAIN_ID} as dmn on (dmn.meta_name_sk = ttf.meta_name_sk and ttf.meta_name_sk >0)
WHERE (dd.timestamp > (UNIX_TIMESTAMP(DATE_SUB(FROM_UNIXTIME(?), INTERVAL 1 MONTH))))
and status = 'success'
group by 1,2,3,4,5
);

insert into transaction_by_name_graph(chain_id, token_id, `timestamp`, meta_name, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	min(dd.timestamp) as final_timestamp,
	dmn.meta_name as meta_name,
	'year' as graph_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_names_${CHAIN_ID} as dmn on (dmn.meta_name_sk = ttf.meta_name_sk and ttf.meta_name_sk >0)	
WHERE (dd.timestamp > (UNIX_TIMESTAMP( DATE_ADD(LAST_DAY(DATE_SUB(FROM_UNIXTIME(?), INTERVAL 1 year)), INTERVAL 1 day))))
and status = 'success'
group by 1,2,4,5
);


</sql>
    <set_params>N</set_params>
    <insert_field />
    <update_field />
    <delete_field />
    <read_field />
    <arguments>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_time_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
    </arguments>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>256</xloc>
      <yloc>272</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step>
    <name>insert_temp_graph_data_meta_type</name>
    <type>ExecSQL</type>
    <description />
    <distribute>N</distribute>
    <custom_distribution />
    <copies>1</copies>
    <partitioning>
      <method>none</method>
      <schema_name />
    </partitioning>
    <connection>analytics_mysql_connection</connection>
    <execute_each_row>Y</execute_each_row>
    <single_statement>N</single_statement>
    <replace_variables>Y</replace_variables>
    <quoteString>N</quoteString>
    <sql>DELETE from transaction_by_type_graph where chain_id=${CHAIN_ID};

insert into transaction_by_type_graph(chain_id, token_id, `timestamp`, meta_type, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	(dd.timestamp + dt.timestamp) as final_timestamp,
	dmt.meta_type as meta_type,
	'day' as graph_duration_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_times dt ON ttf.time_sk = dt.time_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_types as dmt on (dmt.meta_type_sk = ttf.meta_type_sk) and (dmt.meta_type_sk >0)
WHERE (dd.timestamp = ?)
OR    (dd.timestamp = (? - 86400) AND dt.timestamp >= (? + 3600))
and status = 'success'
group by 1,2,3,4,5
);

insert into transaction_by_type_graph(chain_id, token_id, `timestamp`, meta_type, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	dd.timestamp as final_timestamp,
	dmt.meta_type as meta_type,
	'week' as graph_duration_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_types as dmt on (dmt.meta_type_sk = ttf.meta_type_sk) and (dmt.meta_type_sk >0)	
WHERE (dd.timestamp > (? - (86400*7)))
and status = 'success'
group by 1,2,3,4,5
);

insert into transaction_by_type_graph(chain_id, token_id, `timestamp`, meta_type, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	dd.timestamp as final_timestamp,
	dmt.meta_type as meta_type,
	'month' as graph_duration_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_types as dmt on (dmt.meta_type_sk = ttf.meta_type_sk) and (dmt.meta_type_sk >0)	
WHERE (dd.timestamp > (UNIX_TIMESTAMP(DATE_SUB(FROM_UNIXTIME(?), INTERVAL 1 MONTH))))
and status = 'success'
group by 1,2,3,4,5
);

insert into transaction_by_type_graph(chain_id, token_id, `timestamp`, meta_type, graph_duration_type, total_transactions, total_transfers, total_volume)
(SELECT ${CHAIN_ID} as chain_id, 
	 dtkn.token_id,
	min(dd.timestamp) as final_timestamp,
	dmt.meta_type as meta_type,
	'year' as graph_duration_type,
sum(total_transactions) as total_transactions, 
sum(total_transfers) as total_transfers, 
sum(total_volume) as total_volume
FROM token_transfer_facts_${CHAIN_ID} AS ttf
  JOIN dim_tokens_${CHAIN_ID} dtkn ON ttf.token_sk = dtkn.token_sk
  JOIN dim_dates dd ON ttf.date_sk = dd.date_sk
  left join dim_meta_types as dmt on (dmt.meta_type_sk = ttf.meta_type_sk) and (dmt.meta_type_sk >0)
WHERE (dd.timestamp > (UNIX_TIMESTAMP( DATE_ADD(LAST_DAY(DATE_SUB(FROM_UNIXTIME(?), INTERVAL 1 year)), INTERVAL 1 day))))
and status = 'success'
group by 1,2,4,5, dd.month, dd.year
);
</sql>
    <set_params>N</set_params>
    <insert_field />
    <update_field />
    <delete_field />
    <read_field />
    <arguments>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_time_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
      <argument>
        <name>rounded_current_date_timestamp</name>
      </argument>
    </arguments>
    <attributes />
    <cluster_schema />
    <remotesteps>
      <input>
      </input>
      <output>
      </output>
    </remotesteps>
    <GUI>
      <xloc>288</xloc>
      <yloc>96</yloc>
      <draw>Y</draw>
    </GUI>
  </step>
  <step_error_handling>
  </step_error_handling>
  <slave-step-copy-partition-distribution>
  </slave-step-copy-partition-distribution>
  <slave_transformation>N</slave_transformation>
  <attributes />
</transformation>
